{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QxtRkHFzOQi"
      },
      "source": [
        "# Exercises: physics-informed neural network\n",
        "\n",
        "Exercise on the implementation of physics-informed neural network. \n",
        "\n",
        "Date: 2024\n",
        "\n",
        "Course: 056936 - SCIENTIFIC COMPUTING TOOLS FOR ADVANCED MATHEMATICAL MODELLING (PAGANI STEFANO) [2023-24].\n",
        "\n",
        "Example adapted from this [notebook](https://colab.research.google.com/drive/1qBrbgevkSBqqYc8bOPiaoJG1MBrBrluN?usp=share_link). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Let us consider the problem\n",
        "\n",
        "\\begin{aligned}\n",
        "  & -\\nu \\Delta u = -2(x^2+y^2)  \\,, \\quad (x,y) \\in [-1,1] \\times [-1,1]\\,,\\\\\n",
        "  & u(x,-1) = u(x,1) = -x^2 \\,, \\quad -1 < x < 1\\,, \\\\\n",
        "  & u(-1,y) = u(1,y) = -y^2 \\,, \\quad -1 < y < 1\\,, \\\\\n",
        "\\end{aligned}\n",
        "\n",
        "where $\\nu=1$. We consider the PINN framework for reconstructing the solution from sparse measurements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJzSWQcWh85s",
        "outputId": "ee195e0f-9d72-4568-b886-cba89af5b1a1"
      },
      "outputs": [],
      "source": [
        "# import required libraries\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "#!pip -q install pyDOE\n",
        "from pyDOE import lhs  # for latin hypercube sampling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set seed for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\" \n",
        "    Optimize a keras model using scipy.optimize\n",
        "    This block of code is taken and adapted from https://github.com/pedro-r-marques/keras-opt/tree/master\n",
        "    See the repository for all the information.\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import backend as K  # pylint: disable=import-error\n",
        "\n",
        "from tensorflow.python.keras.engine import data_adapter\n",
        "\n",
        "\n",
        "class ScipyOptimizer():\n",
        "    \"\"\" Implements a training function that uses scipy optimize in order\n",
        "        to determine the weights for the model.\n",
        "\n",
        "        The minimize function expects to be able to attempt multiple solutions\n",
        "        over the model. It calls a function which collects all gradients for\n",
        "        all steps and then returns the gradient information to the optimizer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, method='L-BFGS-B', verbose=1, maxiter=1):\n",
        "        self.model = model\n",
        "        self.method = method\n",
        "        self.verbose = verbose\n",
        "        self.maxiter = maxiter\n",
        "        if model.run_eagerly:\n",
        "            self.func = model.__call__\n",
        "        else:\n",
        "            self.func = tf.function(\n",
        "                model.__call__, experimental_relax_shapes=True)\n",
        "\n",
        "    def _update_weights(self, x):\n",
        "        x_offset = 0\n",
        "        for var in self.model.trainable_variables:\n",
        "            shape = var.get_shape()\n",
        "            w_size = np.prod(shape)\n",
        "            value = np.array(x[x_offset:x_offset+w_size]).reshape(shape)\n",
        "            K.set_value(var, value)\n",
        "            x_offset += w_size\n",
        "        assert x_offset == len(x)\n",
        "\n",
        "    def _fun_generator(self, x, iterator):\n",
        "        \"\"\" Function optimized by scipy minimize.\n",
        "\n",
        "            Returns function cost and gradients for all trainable variables.\n",
        "        \"\"\"\n",
        "        model = self.model\n",
        "        self._update_weights(x)\n",
        "        losses = []\n",
        "\n",
        "        dataset = iterator._dataset  # pylint:disable=protected-access\n",
        "        assert dataset is not None\n",
        "        iterator = iter(dataset)\n",
        "\n",
        "        size = dataset.cardinality().numpy()\n",
        "        if size > 0:\n",
        "            n_steps = (size + dataset.batch_size - 1) // dataset.batch_size\n",
        "        else:\n",
        "            n_steps = None\n",
        "\n",
        "        progbar = keras.utils.Progbar(n_steps, verbose=self.verbose)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            for step, data in enumerate(iterator):\n",
        "                data = data_adapter.expand_1d(data)\n",
        "                x, y, sample_weight = data_adapter.unpack_x_y_sample_weight(\n",
        "                    data)\n",
        "                y_pred = self.func(x, training=True)\n",
        "                loss = model.compiled_loss(y, y_pred, sample_weight,\n",
        "                                           regularization_losses=model.losses)\n",
        "                progbar.update(step, [('loss', loss.numpy())])\n",
        "                losses.append(loss)\n",
        "            xloss = tf.reduce_mean(tf.stack(losses))\n",
        "            grads = tape.gradient(xloss, model.trainable_variables)\n",
        "\n",
        "        cost = xloss.numpy()\n",
        "\n",
        "        if all(isinstance(x, tf.Tensor) for x in grads):\n",
        "            xgrads = np.concatenate([x.numpy().reshape(-1) for x in grads])\n",
        "            return cost, xgrads\n",
        "\n",
        "        if all(isinstance(x, tf.IndexedSlices) for x in grads):\n",
        "            xgrad_list = []\n",
        "            for var, grad in zip(model.trainable_variables, grads):\n",
        "                value = tf.Variable(np.zeros(var.shape), dtype=var.dtype)\n",
        "                value.assign_add(grad)\n",
        "                xgrad_list.append(value.numpy())\n",
        "            xgrads = np.concatenate([x.reshape(-1) for x in xgrad_list])\n",
        "            return cost, xgrads\n",
        "\n",
        "        raise NotImplementedError()\n",
        "        return -1, np.array([])  # pylint:disable=unreachable\n",
        "\n",
        "    def train_function(self, iterator):\n",
        "        \"\"\" Called by model fit.\n",
        "        \"\"\"\n",
        "        min_options = {\n",
        "            'maxiter': self.maxiter,\n",
        "            'disp': bool(self.verbose),\n",
        "        }\n",
        "\n",
        "        var_list = self.model.trainable_variables\n",
        "        x0 = np.concatenate([x.numpy().reshape(-1) for x in var_list])\n",
        "\n",
        "        result = minimize(\n",
        "            self._fun_generator, x0, method=self.method, jac=True,\n",
        "            options=min_options, args=(iterator,))\n",
        "\n",
        "        self._update_weights(result['x'])\n",
        "        return {'loss': result['fun']}\n",
        "\n",
        "\n",
        "def make_train_function(model, **kwargs):\n",
        "    \"\"\" Returns a function that will be called to train the model.\n",
        "\n",
        "        model._steps_per_execution must be set in order for train function to\n",
        "        be called once per epoch.\n",
        "    \"\"\"\n",
        "    model._assert_compile_was_called()  # pylint:disable=protected-access\n",
        "    model._configure_steps_per_execution(tf.int64.max)  # pylint:disable=protected-access\n",
        "    opt = ScipyOptimizer(model, **kwargs)\n",
        "    return opt.train_function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot exact solution\n",
        "nu = 1.0 # diffusion parameter\n",
        "\n",
        "# define grid for sampling the exact solution\n",
        "N_h = 128\n",
        "ux = np.linspace(-1.0,1.0,N_h)\n",
        "uy = np.linspace (-1.0,1.0,N_h)\n",
        "\n",
        "# sampling loop:\n",
        "u_true = np.zeros([N_h,N_h])\n",
        "for i in range(N_h):\n",
        "    u_true[:,i] = (ux[i]**2)*(uy**2)\n",
        "\n",
        "X_plot,Y_plot = np.meshgrid(ux,uy)\n",
        "plt.pcolor(X_plot, Y_plot, u_true)\n",
        "plt.axis('equal')\n",
        "plt.colorbar()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# flatten grid and solution\n",
        "X,Y = np.meshgrid(ux,uy)\n",
        "X_flat = tf.convert_to_tensor(np.hstack((X.flatten()[:,None],Y.flatten()[:,None])),dtype=tf.float64)\n",
        "u_flat = u_true.T.flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.shape(u_true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqZ_fC-1KON_"
      },
      "outputs": [],
      "source": [
        "\n",
        "# PINN loss function\n",
        "def PINNloss(xcl,ycl,Xmeas,umeas):\n",
        "    #umeas_pred = PINN(tf.concat([xmeas,ymeas],1))\n",
        "    umeas_pred = PINN(Xmeas)\n",
        "    r_pred   = r_PINN(xcl,ycl)\n",
        "\n",
        "    # loss components\n",
        "    mse_meas  = tf.reduce_mean(tf.pow(umeas-umeas_pred,2))\n",
        "    mse_r  = tf.reduce_mean(tf.pow(r_pred,2))\n",
        "\n",
        "    return mse_r+mse_meas #mse_0+mse_r+mse_lb+mse_ub\n",
        "\n",
        "# neural network weight gradients\n",
        "@tf.function\n",
        "def grad(model,xcl,ycl,xmeas,ymeas,umeas):\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        loss_value = PINNloss(xcl,ycl,tf.concat([xmeas,ymeas],1),umeas)\n",
        "        grads = tape.gradient(loss_value,model.trainable_variables)\n",
        "    return loss_value, grads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXUPXsj0KUK0"
      },
      "outputs": [],
      "source": [
        "# collocation points\n",
        "Ncl = 2000\n",
        "X = lhs(2,Ncl)\n",
        "xcl = tf.expand_dims(tf.cast(-1.0+(2.0)*X[:,0],dtype=tf.float64),axis=-1)\n",
        "ycl = tf.expand_dims(tf.cast(-1.0+(2.0)*X[:,1],dtype=tf.float64),axis=-1)\n",
        "X_coll = tf.concat([xcl,ycl],1)\n",
        "\n",
        "# measurement points\n",
        "\n",
        "Ncl = 20\n",
        "X = lhs(2,Ncl)\n",
        "xmeas = tf.expand_dims(tf.cast(-1.0+(2.0)*X[:,0],dtype=tf.float64),axis=-1)\n",
        "ymeas = tf.expand_dims(tf.cast(-1.0+(2.0)*X[:,1],dtype=tf.float64),axis=-1)\n",
        "X_meas = tf.concat([xmeas,ymeas],1)\n",
        "umeas = (xmeas**2)*(ymeas**2)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Task 1: Test the accuracy of the reconstruction with respect to the optimization configurations and the number of layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "z_cOmQDHKX4k",
        "outputId": "b270d41b-5706-4998-8216-ef717ba49ab1"
      },
      "outputs": [],
      "source": [
        "# training loop\n",
        "\n",
        "# initialize new instance of NN\n",
        "PINN = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(8, activation='tanh', input_shape=(2,),kernel_initializer=\"glorot_normal\",dtype=tf.float64),\n",
        "    tf.keras.layers.Dense(32, activation='tanh',kernel_initializer=\"glorot_normal\",dtype=tf.float64),\n",
        "    tf.keras.layers.Dense(1,activation=None,kernel_initializer=\"glorot_normal\",dtype=tf.float64)\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# residual computation based on AD\n",
        "@tf.function\n",
        "def r_PINN(x,y):\n",
        "    u    = PINN(tf.concat([x,y], 1))\n",
        "    u_x  = tf.gradients(u,x)[0]\n",
        "    u_xx = tf.gradients(u_x, x)[0]\n",
        "    u_y  = tf.gradients(u,y)[0]\n",
        "    u_yy = tf.gradients(u_y, y)[0]\n",
        "    return - u_xx - u_yy + 2.0*( tf.pow(x,2) + tf.pow(y,2) )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # # Adam optimizer\n",
        "# tf_optimizer = tf.keras.optimizers.Adam(learning_rate=0.003,beta_1=0.99)\n",
        "\n",
        "# for iter in range(500):\n",
        "\n",
        "#   # compute gradients using AD\n",
        "#   loss_value,grads = grad(PINN,xcl,ycl,xmeas,ymeas,umeas)\n",
        "\n",
        "  # update neural network weights\n",
        "  #tf_optimizer.apply_gradients(zip(grads,PINN.trainable_variables))\n",
        "\n",
        "#   # display intermediate results\n",
        "#   if ((iter+1) % 200 == 0):\n",
        "#     print('iter =  '+str(iter+1))\n",
        "#     print('loss = {:.4f}'.format(loss_value))\n",
        "#     PINN_flat = PINN(X_flat)\n",
        "#     err = np.linalg.norm(u_flat-PINN_flat[:,-1],2)/np.linalg.norm(u_flat,2)\n",
        "#     print('L2 error: %.4e' % (err))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def my_loss_fn(y_true, y_pred, xcl = xcl, ycl=ycl, xmeas=xmeas, ymeas=ymeas):\n",
        "#     return PINNloss(xcl,ycl,tf.concat([xmeas,ymeas],1),y_true)\n",
        "\n",
        "def my_loss_fn(y_true, y_pred, xcl=xcl, ycl=ycl):\n",
        "    return tf.reduce_mean(tf.pow(y_true - y_pred,2)) + tf.reduce_mean(tf.pow(r_PINN(xcl,ycl),2))\n",
        "\n",
        "print(  my_loss_fn(PINN(tf.concat([xmeas,ymeas],1)) , umeas) )\n",
        "\n",
        "PINN.compile( loss=my_loss_fn, optimizer=tf.keras.optimizers.Adam(learning_rate=0.003,beta_1=0.99) )\n",
        "\n",
        "history_adam = PINN.fit( tf.concat([xmeas,ymeas],1), umeas , epochs=2000 )\n",
        "\n",
        "PINN.compile( loss=my_loss_fn )\n",
        "\n",
        "PINN.train_function = make_train_function(PINN, maxiter=2000)\n",
        "\n",
        "#print(PINN(tf.concat([xmeas,ymeas],1)))\n",
        "#print(umeas)\n",
        "print(PINNloss(xcl,ycl,tf.concat([xmeas,ymeas],1),umeas))\n",
        "\n",
        "history = PINN.fit( tf.concat([xmeas,ymeas],1), umeas )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# # Adam optimizer\n",
        "# tf_optimizer = tf.keras.optimizers.Adam(learning_rate=0.003,beta_1=0.99)\n",
        "\n",
        "# for iter in range(10000):\n",
        "\n",
        "#   # compute gradients using AD\n",
        "#   loss_value,grads = grad(PINN,xcl,ycl,xmeas,ymeas,umeas)\n",
        "\n",
        "#   # update neural network weights\n",
        "#   tf_optimizer.apply_gradients(zip(grads,PINN.trainable_variables))\n",
        "\n",
        "#   # display intermediate results\n",
        "#   if ((iter+1) % 200 == 0):\n",
        "#     print('iter =  '+str(iter+1))\n",
        "#     print('loss = {:.4f}'.format(loss_value))\n",
        "#     PINN_flat = PINN(X_flat)\n",
        "#     err = np.linalg.norm(u_flat-PINN_flat[:,-1],2)/np.linalg.norm(u_flat,2)\n",
        "#     print('L2 error: %.4e' % (err))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "xjbpEPkfWDJe",
        "outputId": "1a86cd1c-64a1-4080-a7cc-1267993fbedc"
      },
      "outputs": [],
      "source": [
        "#Display results\n",
        "PINN_flat = PINN(X_flat)\n",
        "err = np.linalg.norm(u_flat-PINN_flat[:,-1],2)/np.linalg.norm(u_flat,2)\n",
        "\n",
        "fig = plt.figure(figsize=(16,9),dpi=150)\n",
        "#fig = plt.figure()\n",
        "#fig.subplots_adjust(wspace=0.3)\n",
        "plt.style.use('default')\n",
        "ax = fig.add_subplot(1,3,1)\n",
        "ax.set_aspect(1)\n",
        "im = plt.pcolor(X_plot, Y_plot, u_true)\n",
        "plt.scatter(xmeas,ymeas,marker='x',s=10)\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "fig.colorbar(im,cax=cax)\n",
        "#ax.set_yticklabels(['-1.0','-0.6','-0.2','0.2','0.6','1.0'])\n",
        "ax.set_title('Exact Solution',fontsize=16)\n",
        "\n",
        "\n",
        "ax = fig.add_subplot(1,3,2)\n",
        "ax.set_aspect(1)\n",
        "im = plt.pcolor(X_plot, Y_plot, np.reshape(PINN_flat,(N_h,N_h)))\n",
        "plt.scatter(xmeas,ymeas,marker='x',s=10)\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "fig.colorbar(im,cax=cax)\n",
        "ax.set_title('PINN Prediction'.format(err),fontsize=16)\n",
        "\n",
        "ax = fig.add_subplot(1,3,3)\n",
        "ax.set_aspect(1)\n",
        "im = plt.pcolor(X_plot, Y_plot, np.abs( np.reshape(PINN_flat,(N_h,N_h)) -u_true ) )\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "fig.colorbar(im,cax=cax)\n",
        "ax.set_title('L2 error = {:.4f}'.format(err),fontsize=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Task 2: Re-implement the notebook, considering the case where there are no boundary conditions and measurements are taken only on internal points (chosen randomly). Analyze the accuracy with respect to the number of measurements."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
