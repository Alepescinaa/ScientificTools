{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alepescinaa/ScientificTools/blob/main/Project1/Cp3/tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "z-Qdx7crGeGM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09f97008-8239-4cd0-c48d-9f675b357a94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyDOE (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import cvxpy as cvx\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import scipy.io\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "!pip -q install pyDOE\n",
        "from pyDOE import lhs\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Alepescinaa/ScientificTools\n",
        "%cd ScientificTools/Project1/Cp3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNdT-d57GhPu",
        "outputId": "e5f10cb0-baf7-4d1b-a9b4-11b84b352afe"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ScientificTools'...\n",
            "remote: Enumerating objects: 687, done.\u001b[K\n",
            "remote: Counting objects: 100% (86/86), done.\u001b[K\n",
            "remote: Compressing objects: 100% (85/85), done.\u001b[K\n",
            "remote: Total 687 (delta 42), reused 1 (delta 1), pack-reused 601\u001b[K\n",
            "Receiving objects: 100% (687/687), 140.57 MiB | 23.36 MiB/s, done.\n",
            "Resolving deltas: 100% (240/240), done.\n",
            "/content/ScientificTools/Project1/Cp3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jCk9Ral8GeGO"
      },
      "outputs": [],
      "source": [
        "CP3data = np.load(\"CP3data.npz\")\n",
        "CP3data = CP3data['arr_0']\n",
        "\n",
        "CP3estimate = np.load(\"CP3field.npz\")\n",
        "CP3estimate = CP3estimate['arr_0']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mesh_size = 151**2\n",
        "num_samples = 100\n",
        "A = np.zeros((mesh_size, num_samples))\n",
        "\n",
        "for i in range(num_samples):\n",
        "  A[:, i] = CP3estimate[i][0].flatten()\n",
        "\n",
        "[U,s,V] = np.linalg.svd(A, full_matrices=False)\n",
        "\n",
        "k = 10\n",
        "p = 100\n",
        "perm = np.random.choice(mesh_size, size=p, replace=False)\n",
        "\n",
        "C = np.zeros((p,mesh_size))\n",
        "for i in range(p):\n",
        "    C[i,perm[i]] = 1.0\n",
        "\n",
        "Theta = C @ U\n",
        "coeff_mat = np.zeros((k, num_samples))\n",
        "\n",
        "for i in range (num_samples):\n",
        "  u = A[:,i]\n",
        "  y = C @ u\n",
        "  s_c = cvx.Variable(U.shape[1])\n",
        "  constraints = [Theta @ s_c == y]\n",
        "  obj = cvx.Minimize(cvx.norm(s_c, 1))\n",
        "  prob = cvx.Problem(obj, constraints)\n",
        "  prob.solve()\n",
        "  coeff_mat[:, i] = s_c.value[:k]\n",
        "\n",
        "c1_min, c1_max = np.min(coeff_mat[0, :]), np.max(coeff_mat[0, :])\n",
        "c2_min, c2_max = np.min(coeff_mat[1, :]), np.max(coeff_mat[1, :])\n",
        "c3_min, c3_max = np.min(coeff_mat[2, :]), np.max(coeff_mat[2, :])\n",
        "c4_min, c4_max = np.min(coeff_mat[3, :]), np.max(coeff_mat[3, :])\n",
        "c5_min, c5_max = np.min(coeff_mat[4, :]), np.max(coeff_mat[4, :])\n",
        "c6_min, c6_max = np.min(coeff_mat[5, :]), np.max(coeff_mat[5, :])\n",
        "c7_min, c7_max = np.min(coeff_mat[6, :]), np.max(coeff_mat[6, :])\n",
        "c8_min, c8_max = np.min(coeff_mat[7, :]), np.max(coeff_mat[7, :])\n",
        "c9_min, c9_max = np.min(coeff_mat[8, :]), np.max(coeff_mat[8, :])\n",
        "c10_min, c10_max = np.min(coeff_mat[9, :]), np.max(coeff_mat[9, :])\n",
        "\n",
        "delta_c1 = c1_max - c1_min\n",
        "delta_c2 = c2_max - c2_min\n",
        "delta_c3 = c3_max - c3_min\n",
        "delta_c4 = c4_max - c4_min\n",
        "delta_c5 = c5_max - c5_min\n",
        "delta_c6 = c6_max - c6_min\n",
        "delta_c7 = c7_max - c7_min\n",
        "delta_c8 = c8_max - c8_min\n",
        "delta_c9 = c9_max - c9_min\n",
        "delta_c10 = c10_max - c10_min\n",
        "\n",
        "print(f\"c1 in ({c1_min}, {c1_max})\")\n",
        "print(f\"c2 in ({c2_min}, {c2_max})\")\n",
        "print(f\"c3 in ({c3_min}, {c3_max})\")\n",
        "print(f\"c4 in ({c4_min}, {c4_max})\")\n",
        "print(f\"c5 in ({c5_min}, {c5_max})\")\n",
        "print(f\"c6 in ({c6_min}, {c6_max})\")\n",
        "print(f\"c7 in ({c7_min}, {c7_max})\")\n",
        "print(f\"c8 in ({c8_min}, {c8_max})\")\n",
        "print(f\"c9 in ({c9_min}, {c9_max})\")\n",
        "print(f\"c10 in ({c10_min}, {c10_max})\")\n",
        "\n",
        "basis = U[:, :k]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnPdd3cvwaCv",
        "outputId": "be787011-0838-411c-b684-dee0dcebed5b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c1 in (-14901.762094944293, -12144.026767270929)\n",
            "c2 in (-1643.3543282374405, 1582.2151986049146)\n",
            "c3 in (-1666.6222565530495, 1707.6163608834004)\n",
            "c4 in (-776.3212422595338, 1280.5858314852646)\n",
            "c5 in (-1093.9752515213913, 1047.6008504640922)\n",
            "c6 in (-717.0424992189124, 537.6185969065431)\n",
            "c7 in (-469.994381164961, 438.818028217973)\n",
            "c8 in (-328.78631161812, 528.7552626478324)\n",
            "c9 in (-257.8055527291008, 317.60595987413234)\n",
            "c10 in (-216.03993095124412, 357.62370002461455)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# collocation points\n",
        "Ncl = 151**2\n",
        "Xcl = lhs(2,Ncl)\n",
        "xcl = tf.expand_dims(tf.cast(-1.5+(3.0)*Xcl[:,0],dtype=tf.float64),axis=-1)\n",
        "ycl = tf.expand_dims(tf.cast(-1.5+(3.0)*Xcl[:,1],dtype=tf.float64),axis=-1)\n",
        "X_coll = tf.concat([xcl,ycl],1)"
      ],
      "metadata": {
        "id": "gE6IjTv_BqBg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "cvDwkpE9BDRH"
      },
      "outputs": [],
      "source": [
        "def penalty(param, lower_bound, upper_bound):\n",
        "    return tf.reduce_sum(tf.square(tf.maximum(param - upper_bound, 0)) +\n",
        "                         tf.square(tf.maximum(lower_bound - param, 0)))\n",
        "\n",
        "# PINN loss function\n",
        "def loss(xcl,ycl,xmeas,ymeas,umeas,coeff_1,coeff_2,coeff_3,coeff_other,PINN):\n",
        "    input_data=tf.concat([xmeas,ymeas],1)\n",
        "    umeas_pred = PINN(input_data)\n",
        "    r_pred   = r_PINN(xcl,ycl,coeff_1,coeff_2,coeff_3,coeff_other,PINN)\n",
        "\n",
        "    # loss components\n",
        "    mse_meas  = tf.reduce_mean(tf.pow(umeas-umeas_pred,2))\n",
        "    mse_r  = tf.reduce_mean(tf.abs(r_pred))\n",
        "\n",
        "    # bc\n",
        "    y0 = tf.constant([-0.517409965],dtype=tf.float64)\n",
        "    mse_bc= tf.pow( PINN( tf.transpose( tf.stack( [tf.constant([1.5],dtype=tf.float64), y0] ) ) ) ,2)\n",
        "\n",
        "    #penalty\n",
        "    mse_penalty = penalty(coeff_other[0],0,1)+penalty(coeff_other[1],0,1)+penalty(coeff_other[2],0,1)+penalty(coeff_other[3],0,1)+penalty(coeff_other[4],0,1)+penalty(coeff_other[5],0,1)+penalty(coeff_other[6],0,1)+penalty(coeff_1,0,1)+penalty(coeff_2,0,1)+penalty(coeff_3,0,1)\n",
        "\n",
        "    return mse_meas + mse_r + mse_bc + mse_penalty\n",
        "\n",
        "def loss2(xcl,ycl,xmeas,ymeas,umeas,PINN):\n",
        "    input_data=tf.concat([xmeas,ymeas],1)\n",
        "    umeas_pred = PINN(input_data)\n",
        "    r_pred   = r_PINN2(xcl,ycl,PINN)\n",
        "\n",
        "    # loss components\n",
        "    mse_meas  = tf.reduce_mean(tf.pow(umeas-umeas_pred,2))\n",
        "    mse_r  = tf.reduce_mean(tf.abs(r_pred))\n",
        "\n",
        "    # bc\n",
        "    y0 = tf.constant([-0.517409965],dtype=tf.float64)\n",
        "    mse_bc= tf.pow(PINN( tf.transpose( tf.stack( [tf.constant([1.5],dtype=tf.float64), y0] ))  ) ,2)\n",
        "\n",
        "    return mse_meas + mse_r + mse_bc\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def r_PINN(x,y,coeff_1,coeff_2,coeff_3,coeff_other,PINN):\n",
        "    input_data=tf.concat([x,y],1)\n",
        "    u = PINN(input_data)\n",
        "    u_x = tf.gradients(u,x)[0]\n",
        "    u_y = tf.gradients(u,y)[0]\n",
        "    u_grad = tf.transpose(tf.concat([u_x, u_y], axis=1))\n",
        "\n",
        "    pi = tf.constant(np.pi,dtype=tf.float64)\n",
        "    theta_fiber = tf.constant([0.13757033666666668] ,dtype=tf.float64)\n",
        "    a_ratio = tf.constant([5.896298503333333], dtype=tf.float64)\n",
        "    theta0 = pi/2 - theta_fiber\n",
        "\n",
        "    a = tf.stack([tf.cos(theta0), tf.sin(theta0)])\n",
        "    b = tf.stack([tf.cos(theta0-pi/2), tf.sin(theta0-pi/2)])\n",
        "\n",
        "    D_00 = 1 / a_ratio * a[0]**2 + b[0]**2\n",
        "    D_01 = 1 / a_ratio * a[0] * a[1] + b[0] * b[1]\n",
        "    D_10 = 1 / a_ratio * a[0] * a[1] + b[0] * b[1]\n",
        "    D_11 = 1 / a_ratio * a[1]**2 + b[1]**2\n",
        "\n",
        "    c1 = c1_min + delta_c1 * coeff_1\n",
        "    c2 = c2_min + delta_c2 * coeff_2\n",
        "    c3 = c3_min + delta_c3 * coeff_3\n",
        "    c4 = c4_min + delta_c4 * coeff_other[0]\n",
        "    c5 = c5_min + delta_c5 * coeff_other[1]\n",
        "    c6 = c6_min + delta_c6 * coeff_other[2]\n",
        "    c7 = c7_min + delta_c7 * coeff_other[3]\n",
        "    c8 = c8_min + delta_c8 * coeff_other[4]\n",
        "    c9 = c9_min + delta_c9 * coeff_other[5]\n",
        "    c10 = c10_min + delta_c10 * coeff_other[6]\n",
        "\n",
        "    coeff_true = tf.expand_dims(tf.concat([c1[0],c2[0],c3[0],c4,c5,c6,c7,c8,c9,c10], 0), 1)\n",
        "\n",
        "    return   (((u_x * D_00 * u_x + u_x * D_01 * u_y + u_y * D_10 * u_x + u_y * D_11 * u_y)))  - (1/(basis@coeff_true))**2\n",
        "\n",
        "@tf.function\n",
        "def r_PINN2(x,y,PINN):\n",
        "    input_data=tf.concat([x,y],1)\n",
        "    u = PINN(input_data)\n",
        "    u_x = tf.gradients(u,x)[0]\n",
        "    u_y = tf.gradients(u,y)[0]\n",
        "    u_grad = tf.transpose(tf.concat([u_x, u_y], axis=1))\n",
        "\n",
        "    pi = tf.constant(np.pi,dtype=tf.float64)\n",
        "    theta_fiber = tf.constant([0.13757033666666668] ,dtype=tf.float64)\n",
        "    a_ratio = tf.constant([5.896298503333333], dtype=tf.float64)\n",
        "    theta0 = pi/2 - theta_fiber\n",
        "\n",
        "    a = tf.stack([tf.cos(theta0), tf.sin(theta0)])\n",
        "    b = tf.stack([tf.cos(theta0-pi/2), tf.sin(theta0-pi/2)])\n",
        "\n",
        "    D_00 = 1 / a_ratio * a[0]**2 + b[0]**2\n",
        "    D_01 = 1 / a_ratio * a[0] * a[1] + b[0] * b[1]\n",
        "    D_10 = 1 / a_ratio * a[0] * a[1] + b[0] * b[1]\n",
        "    D_11 = 1 / a_ratio * a[1]**2 + b[1]**2\n",
        "\n",
        "    return  (((u_x * D_00 * u_x + u_x * D_01 * u_y + u_y * D_10 * u_x + u_y * D_11 * u_y)))  - (1/100)**2\n",
        "\n",
        "\n",
        "# neural network weight gradients\n",
        "@tf.function\n",
        "def grad(model,xcl,ycl,xmeas,ymeas,umeas,coeff_1,coeff_2,coeff_3,coeff_other):\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        loss_value = loss(xcl,ycl,xmeas,ymeas,umeas,coeff_1,coeff_2,coeff_3,coeff_other,model)\n",
        "        grads = tape.gradient(loss_value,model.trainable_variables)\n",
        "        grads_c1 = tape.gradient(loss_value,coeff_1)\n",
        "        grads_c2 = tape.gradient(loss_value,coeff_2)\n",
        "        grads_c3 = tape.gradient(loss_value,coeff_3)\n",
        "        grads_other = tape.gradient(loss_value,coeff_other)\n",
        "\n",
        "    return loss_value,grads,grads_c1,grads_c2,grads_c3,grads_other\n",
        "    # neural network weight gradients\n",
        "\n",
        "@tf.function\n",
        "def grad2(model,xcl,ycl,xmeas,ymeas,umeas):\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        loss_value2 = loss2(xcl,ycl,xmeas,ymeas,umeas,model)\n",
        "        grads2 = tape.gradient(loss_value2,model.trainable_variables)\n",
        "    return loss_value2, grads2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import regularizers\n",
        "\n",
        "def build_pinn(regularization_strength):\n",
        "    PINN = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(32, activation='relu', input_shape=(2,),\n",
        "                              kernel_initializer=\"glorot_uniform\",\n",
        "                              kernel_regularizer=regularizers.l2(regularization_strength),\n",
        "                              dtype=tf.float64),\n",
        "        tf.keras.layers.Dense(64, activation='relu',\n",
        "                              kernel_initializer=\"glorot_uniform\",\n",
        "                              kernel_regularizer=regularizers.l2(regularization_strength),\n",
        "                              dtype=tf.float64),\n",
        "        tf.keras.layers.Dense(128, activation='relu',\n",
        "                              kernel_initializer=\"glorot_uniform\",\n",
        "                              kernel_regularizer=regularizers.l2(regularization_strength),\n",
        "                              dtype=tf.float64),\n",
        "        tf.keras.layers.Dense(64, activation='relu',\n",
        "                              kernel_initializer=\"glorot_uniform\",\n",
        "                              kernel_regularizer=regularizers.l2(regularization_strength),\n",
        "                              dtype=tf.float64),\n",
        "        tf.keras.layers.Dense(32, activation='relu',\n",
        "                              kernel_initializer=\"glorot_uniform\",\n",
        "                              kernel_regularizer=regularizers.l2(regularization_strength),\n",
        "                              dtype=tf.float64),\n",
        "        tf.keras.layers.Dense(1, activation=None,\n",
        "                              kernel_initializer=\"glorot_uniform\",\n",
        "                              kernel_regularizer=regularizers.l2(regularization_strength),\n",
        "                              dtype=tf.float64)\n",
        "    ])\n",
        "    return PINN\n",
        "\n"
      ],
      "metadata": {
        "id": "tM51pcFCBvdj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def checkpoint2_solution( x, y , t, regularization_strength):\n",
        "\n",
        "  PINN = build_pinn(regularization_strength)\n",
        "\n",
        "  xmeas_train, xmeas_val, ymeas_train, ymeas_val, tmeas_train, tmeas_val = train_test_split(x, y, t, test_size=0.05)\n",
        "  xmeas_train = tf.constant(xmeas_train.reshape(19, 1), dtype=tf.float64)\n",
        "  ymeas_train = tf.constant(ymeas_train.reshape(19, 1), dtype=tf.float64)\n",
        "  tmeas_train = tf.constant(tmeas_train.reshape(19, 1), dtype=tf.float64)\n",
        "  xmeas_val = tf.constant(xmeas_val.reshape(1, 1), dtype=tf.float64)\n",
        "  ymeas_val = tf.constant(ymeas_val.reshape(1, 1), dtype=tf.float64)\n",
        "  tmeas_val = tf.constant(tmeas_val.reshape(1, 1), dtype=tf.float64)\n",
        "\n",
        "  X, Y = np.meshgrid(np.linspace(-1.5,1.5,151), np.linspace(-1.5,1.5,151))\n",
        "\n",
        "  coeff_1 = tf.Variable([[0.5]], trainable=True, dtype=tf.float64)\n",
        "  coeff_2 = tf.Variable([[0.5]], trainable=True, dtype=tf.float64)\n",
        "  coeff_3 = tf.Variable([[0.5]], trainable=True, dtype=tf.float64)\n",
        "  coeff_other = tf.Variable([[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5]], trainable=True, dtype=tf.float64)\n",
        "\n",
        "  tf_optimizer_PINN = tf.keras.optimizers.Adam(learning_rate=0.01,beta_1=0.99)\n",
        "  tf_optimizer_PINN.build(PINN.trainable_variables)\n",
        "  tf_optimizer_c1 = tf.keras.optimizers.Adam(learning_rate=0.01,beta_1=0.99)\n",
        "  tf_optimizer_c1.build([coeff_1])\n",
        "  tf_optimizer_c2 = tf.keras.optimizers.Adam(learning_rate=0.01,beta_1=0.99)\n",
        "  tf_optimizer_c2.build([coeff_2])\n",
        "  tf_optimizer_c3 = tf.keras.optimizers.Adam(learning_rate=0.01,beta_1=0.99)\n",
        "  tf_optimizer_c3.build([coeff_3])\n",
        "  tf_optimizer_other = tf.keras.optimizers.Adam(learning_rate=0.01,beta_1=0.99)\n",
        "  tf_optimizer_other.build([coeff_other])\n",
        "\n",
        "\n",
        "  print()\n",
        "\n",
        "  for iter in range(100):\n",
        "\n",
        "    loss_value2,grads2 = grad2(PINN, xcl,ycl,xmeas_train, ymeas_train, tmeas_train)\n",
        "\n",
        "    tf_optimizer_PINN.apply_gradients(zip(grads2,PINN.trainable_variables))\n",
        "\n",
        "    loss_value_val2, _= grad2(PINN, xcl, ycl, xmeas_val, ymeas_val, tmeas_val)\n",
        "\n",
        "\n",
        "    if ((iter+1) % 100 == 0):\n",
        "      print('iter =  '+str(iter+1))\n",
        "      tf.print('loss =' , loss_value2)\n",
        "      tf.print('loss_val_param =' , loss_value_val2)\n",
        "\n",
        "      c1 = c1_min + delta_c1 * coeff_1\n",
        "      c2 = c2_min + delta_c2 * coeff_2\n",
        "      c3 = c3_min + delta_c3 * coeff_3\n",
        "      c4 = c4_min + delta_c4 * coeff_other[0]\n",
        "      c5 = c5_min + delta_c5 * coeff_other[1]\n",
        "      c6 = c6_min + delta_c6 * coeff_other[2]\n",
        "      c7 = c7_min + delta_c7 * coeff_other[3]\n",
        "      c8 = c8_min + delta_c8 * coeff_other[4]\n",
        "      c9 = c9_min + delta_c9 * coeff_other[5]\n",
        "      c10 = c10_min + delta_c10 * coeff_other[6]\n",
        "\n",
        "      coeff_true = tf.expand_dims(tf.concat([c1[0],c2[0],c3[0],c4,c5,c6,c7,c8,c9,c10], 0), 1)\n",
        "      print(coeff_true.numpy())\n",
        "      print()\n",
        "\n",
        "\n",
        "  for iter in range(100):\n",
        "\n",
        "    loss_value,grads,grads_c1,grads_c2,grads_c3,grads_other = grad(PINN,xcl,ycl,xmeas_train, ymeas_train, tmeas_train,coeff_1,coeff_2,coeff_3,coeff_other)\n",
        "\n",
        "    tf_optimizer_PINN.apply_gradients(zip(grads ,PINN.trainable_variables))\n",
        "    tf_optimizer_c1.apply_gradients(zip([grads_c1], [coeff_1]))\n",
        "    tf_optimizer_c2.apply_gradients(zip([grads_c2], [coeff_2]))\n",
        "    tf_optimizer_c3.apply_gradients(zip([grads_c3], [coeff_3]))\n",
        "    tf_optimizer_other.apply_gradients(zip([grads_other], [coeff_other]))\n",
        "\n",
        "    loss_value_val, _, _, _, _, _ = grad(PINN,xcl,ycl,xmeas_train, ymeas_train, tmeas_train,coeff_1,coeff_2,coeff_3,coeff_other)\n",
        "\n",
        "\n",
        "    if ((iter+1) % 100 == 0):\n",
        "      print('iter =  '+str(iter+1))\n",
        "      tf.print('loss =' , loss_value)\n",
        "      tf.print('loss_val_param =' , loss_value_val)\n",
        "\n",
        "      c1 = c1_min + delta_c1 * coeff_1\n",
        "      c2 = c2_min + delta_c2 * coeff_2\n",
        "      c3 = c3_min + delta_c3 * coeff_3\n",
        "      c4 = c4_min + delta_c4 * coeff_other[0]\n",
        "      c5 = c5_min + delta_c5 * coeff_other[1]\n",
        "      c6 = c6_min + delta_c6 * coeff_other[2]\n",
        "      c7 = c7_min + delta_c7 * coeff_other[3]\n",
        "      c8 = c8_min + delta_c8 * coeff_other[4]\n",
        "      c9 = c9_min + delta_c9 * coeff_other[5]\n",
        "      c10 = c10_min + delta_c10 * coeff_other[6]\n",
        "\n",
        "      coeff_true = tf.expand_dims(tf.concat([c1[0],c2[0],c3[0],c4,c5,c6,c7,c8,c9,c10], 0), 1)\n",
        "      print(coeff_true.numpy())\n",
        "      print()\n",
        "\n",
        "  return regularization_strength, loss_value_val"
      ],
      "metadata": {
        "id": "uST9tU64B9fa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_random_search = 10\n",
        "regularization_strengths = np.logspace(-6, 0, num=num_random_search)\n",
        "regularization_strengths"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f55RDRCWIWVA",
        "outputId": "432a9831-1965-4700-fd01-c9b73c4dfc87"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.00000000e-06, 4.64158883e-06, 2.15443469e-05, 1.00000000e-04,\n",
              "       4.64158883e-04, 2.15443469e-03, 1.00000000e-02, 4.64158883e-02,\n",
              "       2.15443469e-01, 1.00000000e+00])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ind_disp = 64\n",
        "x_meas = CP3data[ind_disp][0]\n",
        "y_meas = CP3data[ind_disp][1]\n",
        "t_meas = CP3data[ind_disp][2]\n",
        "speed_field = CP3estimate[ind_disp][0]\n",
        "\n",
        "num_random_search = 10\n",
        "regularization_strengths = np.logspace(-6, 0, num=num_random_search)\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "best_regularization_strength = None\n",
        "\n",
        "\n",
        "for regularization_strength in regularization_strengths:\n",
        "  regularization_strength, loss_value_val= checkpoint2_solution(x_meas, y_meas, t_meas,regularization_strength)\n",
        "  if  loss_value_val < best_val_loss:\n",
        "      best_val_loss =  loss_value_val\n",
        "      best_regularization_strength = regularization_strength\n",
        "  print(f'Best Regularization strength: {best_regularization_strength}, Best Validation Loss: {best_val_loss}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2YBEKL3FxSI",
        "outputId": "73e74da7-b820-4fdc-ed80-40cff42a2103"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "iter =  100\n",
            "loss = [[0.00022927350193939705]]\n",
            "loss_val_param = [[0.00014616186505241559]]\n",
            "[[-13522.89443111]\n",
            " [   -30.56956482]\n",
            " [    20.49705217]\n",
            " [   252.13229461]\n",
            " [   -23.18720053]\n",
            " [   -89.71195116]\n",
            " [   -15.58817647]\n",
            " [    99.98447551]\n",
            " [    29.90020357]\n",
            " [    70.79188454]]\n",
            "\n",
            "iter =  100\n",
            "loss = [[0.00010212496473648574]]\n",
            "loss_val_param = [[9.5848823474272923e-05]]\n",
            "[[-14833.71281818]\n",
            " [   102.40314222]\n",
            " [  -113.76443917]\n",
            " [  -700.99946631]\n",
            " [    53.76101098]\n",
            " [   114.93393587]\n",
            " [    47.61562686]\n",
            " [    75.48327332]\n",
            " [    65.7306727 ]\n",
            " [  -101.16013079]]\n",
            "\n",
            "Best Regularization strength: 1e-06, Best Validation Loss: [[9.58488235e-05]]\n",
            "\n",
            "iter =  100\n",
            "loss = [[6.4020377338690229e-05]]\n",
            "loss_val_param = [[0.00010561402172357465]]\n",
            "[[-13522.89443111]\n",
            " [   -30.56956482]\n",
            " [    20.49705217]\n",
            " [   252.13229461]\n",
            " [   -23.18720053]\n",
            " [   -89.71195116]\n",
            " [   -15.58817647]\n",
            " [    99.98447551]\n",
            " [    29.90020357]\n",
            " [    70.79188454]]\n",
            "\n",
            "iter =  100\n",
            "loss = [[4.5276167254546928e-05]]\n"
          ]
        }
      ]
    }
  ]
}